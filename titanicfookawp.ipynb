{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-29T10:21:23.812261Z",
     "iopub.status.busy": "2025-10-29T10:21:23.812022Z",
     "iopub.status.idle": "2025-10-29T10:21:24.572301Z",
     "shell.execute_reply": "2025-10-29T10:21:24.571407Z",
     "shell.execute_reply.started": "2025-10-29T10:21:23.812242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "file_train_data = \"data/train.csv\"\n",
    "file_test_data = \"data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T10:30:25.431024Z",
     "iopub.status.busy": "2025-10-29T10:30:25.430630Z",
     "iopub.status.idle": "2025-10-29T10:30:25.442571Z",
     "shell.execute_reply": "2025-10-29T10:30:25.441503Z",
     "shell.execute_reply.started": "2025-10-29T10:30:25.430994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'PassengerId': '1', 'Survived': '0', 'Pclass': '3', 'Name': 'Braund, Mr. Owen Harris', 'Sex': 'male', 'Age': '22', 'SibSp': '1', 'Parch': '0', 'Ticket': 'A/5 21171', 'Fare': '7.25', 'Cabin': '', 'Embarked': 'S'}, {'PassengerId': '2', 'Survived': '1', 'Pclass': '1', 'Name': 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)', 'Sex': 'female', 'Age': '38', 'SibSp': '1', 'Parch': '0', 'Ticket': 'PC 17599', 'Fare': '71.2833', 'Cabin': 'C85', 'Embarked': 'C'}, {'PassengerId': '3', 'Survived': '1', 'Pclass': '3', 'Name': 'Heikkinen, Miss. Laina', 'Sex': 'female', 'Age': '26', 'SibSp': '0', 'Parch': '0', 'Ticket': 'STON/O2. 3101282', 'Fare': '7.925', 'Cabin': '', 'Embarked': 'S'}]\n",
      "[{'PassengerId': '892', 'Pclass': '3', 'Name': 'Kelly, Mr. James', 'Sex': 'male', 'Age': '34.5', 'SibSp': '0', 'Parch': '0', 'Ticket': '330911', 'Fare': '7.8292', 'Cabin': '', 'Embarked': 'Q'}, {'PassengerId': '893', 'Pclass': '3', 'Name': 'Wilkes, Mrs. James (Ellen Needs)', 'Sex': 'female', 'Age': '47', 'SibSp': '1', 'Parch': '0', 'Ticket': '363272', 'Fare': '7', 'Cabin': '', 'Embarked': 'S'}, {'PassengerId': '894', 'Pclass': '2', 'Name': 'Myles, Mr. Thomas Francis', 'Sex': 'male', 'Age': '62', 'SibSp': '0', 'Parch': '0', 'Ticket': '240276', 'Fare': '9.6875', 'Cabin': '', 'Embarked': 'Q'}]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "train_csv = []\n",
    "with open(file_train_data, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    for row in reader:\n",
    "        train_csv.append(row)\n",
    "\n",
    "print(train_csv[:3])\n",
    "\n",
    "test_csv = []\n",
    "with open(file_test_data, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    for row in reader:\n",
    "        test_csv.append(row)\n",
    "\n",
    "print(test_csv[:3])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T10:42:10.956467Z",
     "iopub.status.busy": "2025-10-29T10:42:10.956171Z",
     "iopub.status.idle": "2025-10-29T10:42:10.965620Z",
     "shell.execute_reply": "2025-10-29T10:42:10.964842Z",
     "shell.execute_reply.started": "2025-10-29T10:42:10.956447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '1', 'input': [1.0, 0.0, 0.2711736617240513, 0.125, 0.0, 0.014151057562208049, 0.0, 0.0], 'output': [0.0]}, {'id': '2', 'input': [0.0, 1.0, 0.4722292033174164, 0.125, 0.0, 0.13913573538264068, 1.0, 0.0], 'output': [1.0]}, {'id': '3', 'input': [1.0, 1.0, 0.32143754712239253, 0.0, 0.0, 0.015468569817999833, 0.0, 0.0], 'output': [1.0]}]\n",
      "[{'id': '892', 'input': [1.0, 0.0, 0.45272319662402744, 0.0, 0.0, 0.015281580671177828, 0.0, 0.0], 'output': [0.0]}, {'id': '893', 'input': [1.0, 1.0, 0.6175656072794409, 0.125, 0.0, 0.013663090060062943, 0.0, 0.0], 'output': [0.0]}, {'id': '894', 'input': [0.5, 0.0, 0.815376500065937, 0.0, 0.0, 0.018908740708122825, 0.0, 0.0], 'output': [0.0]}]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "special_titles = [\"Dr\", \"Rev\", \"Col\", \"Major\", \"Sir\", \"Lady\"]\n",
    "\n",
    "def parse_data(lines):\n",
    "    # First we filter out the data we need\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        data.append(\n",
    "            {\n",
    "                \"id\": l[\"PassengerId\"],\n",
    "                \"survived\": l.get(\"Survived\", 0),\n",
    "                \"class\": l[\"Pclass\"],\n",
    "                \"sex\": l[\"Sex\"],\n",
    "                \"age\": l[\"Age\"],\n",
    "                \"sibsp\": l[\"SibSp\"],\n",
    "                \"parch\": l[\"Parch\"],\n",
    "                \"fare\": l.get(\"Fare\", 0),\n",
    "                # \"embarked\": l[\"Embarked\"], # Seems irrelevant\n",
    "                \"has_cabin\": 0 if l[\"Cabin\"] == \"\" else 1,\n",
    "                \"has_title\": 1 if any(title in l[\"Name\"] for title in special_titles) else 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Normalize data\n",
    "    norm_sex = {\"male\": 0, \"female\": 1}\n",
    "    norm_embarked = {\"S\": 0, \"C\": 1, \"Q\": 2, \"\": 3}\n",
    "\n",
    "    normalized_data = []\n",
    "    # Numberize values\n",
    "    for row in data:\n",
    "        # Check for errors\n",
    "        if norm_sex.get(row[\"sex\"], \"unknown\") == \"unknown\":\n",
    "            print(row[\"id\"], row[\"sex\"])\n",
    "        # if norm_embarked.get(row[\"embarked\"], \"unknown\") == \"unknown\":\n",
    "        #     print(row[\"id\"], row[\"embarked\"])\n",
    "\n",
    "        nd = copy.deepcopy(row)\n",
    "\n",
    "        nd[\"sex\"] = norm_sex[row[\"sex\"]]\n",
    "        nd[\"fare\"] = float(row[\"fare\"]) if row[\"fare\"] != \"\" else 0\n",
    "        # nd[\"embarked\"] = norm_embarked[row[\"embarked\"]]\n",
    "        nd[\"age\"] = float(row[\"age\"]) if row[\"age\"] != \"\" else 0\n",
    "\n",
    "        normalized_data.append(nd)\n",
    "\n",
    "    # Add median age if age is 0\n",
    "    ages = [d[\"age\"] for d in normalized_data if d[\"age\"] != 0]\n",
    "    median_age = sorted(ages)[len(ages) // 2]\n",
    "    for r in normalized_data:\n",
    "        if r[\"age\"] == 0:\n",
    "            r[\"age\"] = median_age\n",
    "\n",
    "\n",
    "    keys = list(normalized_data[0].keys())\n",
    "\n",
    "    # Extract min max of each key\n",
    "    keys_minmax = {}\n",
    "    for k in keys:\n",
    "        values = [d[k] for d in normalized_data if k in d]\n",
    "        # print(values)\n",
    "        min_value, max_value = float(min(values)), float(max(values))\n",
    "        keys_minmax[k] = {\"min\": min_value, \"max\": max_value}\n",
    "\n",
    "    # Normalize 0 to 1 all keys\n",
    "    for r in normalized_data:\n",
    "        for k in keys:\n",
    "            if k == \"id\":\n",
    "                continue\n",
    "            value = float(r[k])\n",
    "            minmax = keys_minmax[k]\n",
    "            norm_value = (value - minmax[\"min\"]) / (minmax[\"max\"] - minmax[\"min\"]) if (minmax[\"max\"] - minmax[\"min\"]) != 0 else 0.0\n",
    "            r[k] = norm_value\n",
    "            # print(keys_minmax[k], value, norm_value)\n",
    "\n",
    "    # Split data into input -> expected output\n",
    "    train_data = []\n",
    "    for r in normalized_data:\n",
    "        train_data.append(\n",
    "            {\n",
    "                \"id\": r[\"id\"],\n",
    "                \"input\": list(\n",
    "                    {k: v for k, v in r.items() if k not in [\"survived\", \"id\"]}.values()\n",
    "                ),\n",
    "                \"output\": [r[\"survived\"]],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return train_data\n",
    "\n",
    "train_data = parse_data(train_csv)\n",
    "print(train_data[:3])\n",
    "\n",
    "test_data = parse_data(test_csv)\n",
    "print(test_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T11:11:45.045237Z",
     "iopub.status.busy": "2025-10-29T11:11:45.044884Z",
     "iopub.status.idle": "2025-10-29T11:11:45.056497Z",
     "shell.execute_reply": "2025-10-29T11:11:45.055453Z",
     "shell.execute_reply.started": "2025-10-29T11:11:45.045206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Determine the best available device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = get_device()\n",
    "print(DEVICE)\n",
    "\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    \"\"\"Simple feedforward neural network using PyTorch\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 16,\n",
    "        hidden_layers: List[int] = [256],\n",
    "        output_size: int = 4,\n",
    "        empty: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if empty:\n",
    "            return\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Build layers using PyTorch modules\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Add hidden layers\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            # layers.append(nn.Sigmoid())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Add output layer (no activation)\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights using He initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "        # Move to device\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, nonlinearity=\"tanh\")\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Convert numpy array to tensor if needed and move to device\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x).float().to(DEVICE)\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            x = x.to(DEVICE)\n",
    "        elif isinstance(x, list):\n",
    "            x = torch.tensor(x, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        return self.network(x)\n",
    "\n",
    "    def mutate(self, mutation_rate: float = 0.1, mutation_strength: float = 0.5):\n",
    "        \"\"\"Mutate the network's weights and biases\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                if torch.rand(1).item() < mutation_rate:\n",
    "                    mutation = torch.randn_like(param) * mutation_strength\n",
    "                    param.add_(mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class EvolutionaryOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        population_size: int = 50,\n",
    "        elite_size: int = 10,\n",
    "        new_members: int = 0,\n",
    "        mutation_rate: float = 0.1,\n",
    "        mutation_strength: float = 0.5,\n",
    "        input_size: int = 8,\n",
    "        hidden_layers: List[int] = [32],\n",
    "        output_size: int = 1,\n",
    "        train_data: List = [],\n",
    "    ):\n",
    "        self.population_size = population_size\n",
    "        self.elite_size = elite_size\n",
    "        self.new_members = new_members\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.mutation_strength = mutation_strength\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.train_data = train_data\n",
    "\n",
    "        # Convert training data to tensors once\n",
    "        self.input_tensor = torch.tensor([data['input'] for data in train_data], dtype=torch.float32).to(DEVICE)\n",
    "        self.output_tensor = torch.tensor([data['output'] for data in train_data], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        # Create initial population\n",
    "        self.population = []\n",
    "        for _ in range(population_size):\n",
    "            network = SimpleNeuralNetwork(\n",
    "                input_size=input_size,\n",
    "                output_size=output_size,\n",
    "                hidden_layers=hidden_layers,\n",
    "            )\n",
    "            self.population.append(network)\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "    ) -> List[Tuple[SimpleNeuralNetwork, int, float]]:\n",
    "        if DEVICE.type == \"cuda\" or DEVICE.type == \"mps\":\n",
    "            # GPU evaluation - process all networks sequentially since GPU handles parallelization\n",
    "            results = []\n",
    "            for network in self.population:\n",
    "                network.eval()\n",
    "                with torch.no_grad():\n",
    "                    predictions = network(self.input_tensor)\n",
    "                    predictions = (predictions > 0.5).float()\n",
    "                    accuracy = (predictions == self.output_tensor).float().mean().item()\n",
    "                    results.append((network, accuracy))\n",
    "            return results\n",
    "        else:\n",
    "            # CPU evaluation - use joblib for parallel processing\n",
    "            def eval_network(network: SimpleNeuralNetwork):\n",
    "                network.eval()\n",
    "                scores = []\n",
    "                with torch.no_grad():\n",
    "                    for test in self.train_data:\n",
    "                        input_values = test['input']\n",
    "                        prediction = network.forward(input_values)\n",
    "                        prediction = True if prediction[0] > .5 else False\n",
    "                        reality = True if test['output'][0] == 1.0 else False\n",
    "                        scores.append(1 if prediction == reality else 0)\n",
    "                return (network, sum(scores) / len(scores))\n",
    "\n",
    "            results = Parallel(n_jobs=joblib.cpu_count())(\n",
    "                delayed(eval_network)(net) for net in self.population\n",
    "            )\n",
    "            return results\n",
    "\n",
    "    def select_and_breed(\n",
    "        self, evaluated: List[Tuple[SimpleNeuralNetwork, int, float]]\n",
    "    ) -> None:\n",
    "        # Sort by score descending\n",
    "        evaluated.sort(key=lambda x: x[1], reverse=True)\n",
    "        elite = evaluated[: self.elite_size]\n",
    "\n",
    "        new_population = []\n",
    "        # Keep elite networks\n",
    "        for net, _ in elite:\n",
    "            new_population.append(net)\n",
    "\n",
    "        # Create offspring by mutating elite networks\n",
    "        while len(new_population) < self.population_size:\n",
    "            parent = random.choice(elite)[0]\n",
    "\n",
    "            # Create a child by copying the parent's state\n",
    "            child = copy.deepcopy(parent)\n",
    "\n",
    "            # Mutate the child\n",
    "            child.mutate(self.mutation_rate, self.mutation_strength)\n",
    "            new_population.append(child)\n",
    "\n",
    "        # Add random new members\n",
    "        for _ in range(self.new_members):\n",
    "            network = SimpleNeuralNetwork(hidden_layers=self.hidden_layers, input_size=self.input_size, output_size=self.output_size)\n",
    "            new_population.append(network)\n",
    "\n",
    "        self.population = new_population\n",
    "\n",
    "    def run_generation(\n",
    "        self,\n",
    "    ) -> Tuple[List[SimpleNeuralNetwork], float]:\n",
    "        evaluated = self.evaluate()\n",
    "        best_precision = max(precision for _, precision in evaluated)\n",
    "\n",
    "        self.select_and_breed(evaluated)\n",
    "\n",
    "        return self.population, best_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_network(network: SimpleNeuralNetwork, filename: str):\n",
    "    torch.save(network.state_dict(), filename)\n",
    "\n",
    "\n",
    "def load_network(filename: str, hidden_layers: List[int]) -> SimpleNeuralNetwork:\n",
    "    network = SimpleNeuralNetwork(hidden_layers=hidden_layers)\n",
    "    network.load_state_dict(torch.load(filename, map_location=DEVICE))\n",
    "    network.to(DEVICE)\n",
    "    return network\n",
    "\n",
    "\n",
    "def save_population(population: List[SimpleNeuralNetwork], filename: str):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(population, f)\n",
    "\n",
    "\n",
    "def load_population(filename: str) -> List[SimpleNeuralNetwork]:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        population = pickle.load(f)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:19:47.931825Z",
     "iopub.status.busy": "2025-10-29T12:19:47.931490Z",
     "iopub.status.idle": "2025-10-29T12:19:47.939072Z",
     "shell.execute_reply": "2025-10-29T12:19:47.938169Z",
     "shell.execute_reply.started": "2025-10-29T12:19:47.931801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving networks to folder: networks/128_64_32\n",
      "Epoch 1 starting...\n",
      "Epoch 1 completed after 0:02:18.434214\n",
      "Saved population for epoch 1 with precision 0.8305\n",
      "Epoch 2 starting...\n",
      "Epoch 2 completed after 0:04:37.651804\n",
      "Epoch 3 starting...\n",
      "Epoch 3 completed after 0:06:58.523072\n",
      "Saved population for epoch 3 with precision 0.8316\n",
      "Epoch 4 starting...\n",
      "Epoch 4 completed after 0:09:21.948226\n",
      "Epoch 5 starting...\n",
      "Epoch 5 completed after 0:11:43.579369\n",
      "Saved population for epoch 5 with precision 0.8328\n",
      "Epoch 6 starting...\n",
      "Epoch 6 completed after 0:14:05.288253\n",
      "Epoch 7 starting...\n",
      "Epoch 7 completed after 0:16:24.601955\n",
      "Epoch 8 starting...\n",
      "Epoch 8 completed after 0:18:41.461614\n",
      "Epoch 9 starting...\n",
      "Epoch 9 completed after 0:20:58.594210\n",
      "Epoch 10 starting...\n",
      "Epoch 10 completed after 0:23:15.260761\n",
      "Epoch 11 starting...\n",
      "Epoch 11 completed after 0:25:31.725006\n",
      "Epoch 12 starting...\n",
      "Epoch 12 completed after 0:27:47.876725\n",
      "Epoch 13 starting...\n",
      "Epoch 13 completed after 0:30:05.399141\n",
      "Epoch 14 starting...\n",
      "Epoch 14 completed after 0:32:27.158345\n",
      "Epoch 15 starting...\n",
      "Epoch 15 completed after 0:34:47.965734\n",
      "Epoch 16 starting...\n",
      "Epoch 16 completed after 0:37:04.603579\n",
      "Epoch 17 starting...\n",
      "Epoch 17 completed after 0:39:21.016161\n",
      "Epoch 18 starting...\n",
      "Epoch 18 completed after 0:41:37.874112\n",
      "Epoch 19 starting...\n",
      "Epoch 19 completed after 0:43:55.386228\n",
      "Epoch 20 starting...\n",
      "Epoch 20 completed after 0:46:12.715397\n",
      "Epoch 21 starting...\n",
      "Epoch 21 completed after 0:48:30.820447\n",
      "Epoch 22 starting...\n",
      "Epoch 22 completed after 0:50:47.093790\n",
      "Epoch 23 starting...\n",
      "Epoch 23 completed after 0:53:07.006159\n",
      "Epoch 24 starting...\n",
      "Epoch 24 completed after 0:55:26.157566\n",
      "Epoch 25 starting...\n",
      "Epoch 25 completed after 0:57:47.673983\n",
      "Epoch 26 starting...\n",
      "Epoch 26 completed after 1:00:08.356049\n",
      "Epoch 27 starting...\n",
      "Epoch 27 completed after 1:02:25.676667\n",
      "Epoch 28 starting...\n",
      "Epoch 28 completed after 1:04:41.453608\n",
      "Epoch 29 starting...\n",
      "Epoch 29 completed after 1:06:58.013138\n",
      "Epoch 30 starting...\n",
      "Epoch 30 completed after 1:09:13.851374\n",
      "Epoch 31 starting...\n",
      "Epoch 31 completed after 1:11:31.568836\n",
      "Epoch 32 starting...\n",
      "Epoch 32 completed after 1:13:52.088235\n",
      "Epoch 33 starting...\n",
      "Epoch 33 completed after 1:16:08.774793\n",
      "Epoch 34 starting...\n",
      "Epoch 34 completed after 1:18:25.032195\n",
      "Epoch 35 starting...\n",
      "Epoch 35 completed after 1:20:41.655596\n",
      "Epoch 36 starting...\n",
      "Epoch 36 completed after 1:22:58.883874\n",
      "Epoch 37 starting...\n",
      "Epoch 37 completed after 1:25:15.355064\n",
      "Epoch 38 starting...\n",
      "Epoch 38 completed after 1:27:31.478512\n",
      "Saved population for epoch 38 with precision 0.8350\n",
      "Epoch 39 starting...\n",
      "Epoch 39 completed after 1:29:49.702170\n",
      "Epoch 40 starting...\n",
      "Epoch 40 completed after 1:32:08.471715\n",
      "Epoch 41 starting...\n",
      "Epoch 41 completed after 1:34:26.034059\n",
      "Epoch 42 starting...\n",
      "Epoch 42 completed after 1:36:42.595551\n",
      "Epoch 43 starting...\n",
      "Epoch 43 completed after 1:38:59.727545\n",
      "Epoch 44 starting...\n",
      "Epoch 44 completed after 1:41:16.142184\n",
      "Epoch 45 starting...\n",
      "Epoch 45 completed after 1:43:32.795837\n",
      "Epoch 46 starting...\n",
      "Epoch 46 completed after 1:45:49.155917\n",
      "Epoch 47 starting...\n",
      "Epoch 47 completed after 1:48:05.762236\n",
      "Epoch 48 starting...\n",
      "Epoch 48 completed after 1:50:21.923436\n",
      "Epoch 49 starting...\n",
      "Epoch 49 completed after 1:52:40.717726\n",
      "Epoch 50 starting...\n",
      "Epoch 50 completed after 1:55:00.355311\n",
      "Epoch 51 starting...\n",
      "Epoch 51 completed after 1:57:19.453479\n",
      "Epoch 52 starting...\n",
      "Epoch 52 completed after 1:59:40.799221\n",
      "Epoch 53 starting...\n",
      "Epoch 53 completed after 2:02:03.815530\n",
      "Epoch 54 starting...\n",
      "Epoch 54 completed after 2:04:22.463592\n",
      "Epoch 55 starting...\n",
      "Epoch 55 completed after 2:06:45.510597\n",
      "Epoch 56 starting...\n",
      "Epoch 56 completed after 2:09:08.363331\n",
      "Epoch 57 starting...\n",
      "Epoch 57 completed after 2:11:31.211087\n",
      "Epoch 58 starting...\n",
      "Epoch 58 completed after 2:13:51.296656\n",
      "Epoch 59 starting...\n",
      "Epoch 59 completed after 2:16:09.227494\n",
      "Epoch 60 starting...\n",
      "Epoch 60 completed after 2:18:31.321732\n",
      "Epoch 61 starting...\n",
      "Epoch 61 completed after 2:20:53.676188\n",
      "Epoch 62 starting...\n",
      "Epoch 62 completed after 2:23:15.299607\n",
      "Epoch 63 starting...\n",
      "Epoch 63 completed after 2:25:36.294189\n",
      "Epoch 64 starting...\n",
      "Epoch 64 completed after 2:27:59.283584\n",
      "Epoch 65 starting...\n",
      "Epoch 65 completed after 2:30:17.090218\n",
      "Epoch 66 starting...\n",
      "Epoch 66 completed after 2:32:35.458714\n",
      "Epoch 67 starting...\n",
      "Epoch 67 completed after 2:34:52.908653\n",
      "Epoch 68 starting...\n",
      "Epoch 68 completed after 2:37:12.000546\n",
      "Epoch 69 starting...\n",
      "Epoch 69 completed after 2:39:31.198031\n",
      "Epoch 70 starting...\n",
      "Epoch 70 completed after 2:41:48.668945\n",
      "Epoch 71 starting...\n",
      "Epoch 71 completed after 2:44:06.069278\n",
      "Epoch 72 starting...\n",
      "Epoch 72 completed after 2:46:23.181622\n",
      "Epoch 73 starting...\n",
      "Epoch 73 completed after 2:48:41.908566\n",
      "Epoch 74 starting...\n",
      "Epoch 74 completed after 2:51:01.614869\n",
      "Epoch 75 starting...\n",
      "Epoch 75 completed after 2:53:21.876500\n",
      "Epoch 76 starting...\n",
      "Epoch 76 completed after 2:55:42.081021\n",
      "Epoch 77 starting...\n",
      "Epoch 77 completed after 2:57:59.699722\n",
      "Epoch 78 starting...\n",
      "Epoch 78 completed after 3:00:16.982746\n",
      "Epoch 79 starting...\n",
      "Epoch 79 completed after 3:02:34.820762\n",
      "Epoch 80 starting...\n",
      "Epoch 80 completed after 3:04:52.498304\n",
      "Epoch 81 starting...\n",
      "Epoch 81 completed after 3:07:09.459347\n",
      "Epoch 82 starting...\n",
      "Epoch 82 completed after 3:09:26.469863\n",
      "Epoch 83 starting...\n",
      "Epoch 83 completed after 3:11:43.948694\n",
      "Epoch 84 starting...\n",
      "Epoch 84 completed after 3:14:03.265037\n",
      "Epoch 85 starting...\n",
      "Epoch 85 completed after 3:16:20.634101\n",
      "Epoch 86 starting...\n",
      "Epoch 86 completed after 3:18:37.435223\n",
      "Epoch 87 starting...\n",
      "Epoch 87 completed after 3:20:56.884704\n",
      "Epoch 88 starting...\n",
      "Epoch 88 completed after 3:23:16.872736\n",
      "Epoch 89 starting...\n",
      "Epoch 89 completed after 3:25:38.142965\n",
      "Epoch 90 starting...\n",
      "Epoch 90 completed after 3:28:00.555551\n",
      "Epoch 91 starting...\n",
      "Epoch 91 completed after 3:30:18.645181\n",
      "Epoch 92 starting...\n",
      "Epoch 92 completed after 3:32:39.798469\n",
      "Epoch 93 starting...\n",
      "Epoch 93 completed after 3:34:59.214933\n",
      "Epoch 94 starting...\n",
      "Epoch 94 completed after 3:37:16.126183\n",
      "Epoch 95 starting...\n",
      "Epoch 95 completed after 3:39:33.294680\n",
      "Epoch 96 starting...\n",
      "Epoch 96 completed after 3:41:49.634058\n",
      "Epoch 97 starting...\n",
      "Epoch 97 completed after 3:44:05.648237\n",
      "Epoch 98 starting...\n",
      "Epoch 98 completed after 3:46:21.838938\n",
      "Epoch 99 starting...\n",
      "Epoch 99 completed after 3:48:38.011757\n",
      "Epoch 100 starting...\n",
      "Epoch 100 completed after 3:50:56.123561\n",
      "Epoch 101 starting...\n",
      "Epoch 101 completed after 3:53:12.698014\n",
      "Epoch 102 starting...\n",
      "Epoch 102 completed after 3:55:28.556805\n",
      "Epoch 103 starting...\n",
      "Epoch 103 completed after 3:57:46.953251\n",
      "Epoch 104 starting...\n",
      "Epoch 104 completed after 4:00:06.085879\n",
      "Epoch 105 starting...\n",
      "Epoch 105 completed after 4:02:22.373965\n",
      "Epoch 106 starting...\n",
      "Epoch 106 completed after 4:04:38.643952\n",
      "Epoch 107 starting...\n",
      "Epoch 107 completed after 4:06:56.277269\n",
      "Epoch 108 starting...\n",
      "Epoch 108 completed after 4:09:13.892986\n",
      "Epoch 109 starting...\n",
      "Epoch 109 completed after 4:11:31.313497\n",
      "Epoch 110 starting...\n",
      "Epoch 110 completed after 4:13:48.911026\n",
      "Epoch 111 starting...\n",
      "Epoch 111 completed after 4:16:06.626976\n",
      "Epoch 112 starting...\n",
      "Epoch 112 completed after 4:18:24.543465\n",
      "Epoch 113 starting...\n",
      "Epoch 113 completed after 4:20:42.167607\n",
      "Epoch 114 starting...\n",
      "Epoch 114 completed after 4:22:59.155555\n",
      "Epoch 115 starting...\n",
      "Epoch 115 completed after 4:25:18.274994\n",
      "Epoch 116 starting...\n",
      "Epoch 116 completed after 4:27:36.190545\n",
      "Epoch 117 starting...\n",
      "Epoch 117 completed after 4:29:53.393108\n",
      "Epoch 118 starting...\n",
      "Epoch 118 completed after 4:32:11.905358\n",
      "Epoch 119 starting...\n",
      "Epoch 119 completed after 4:34:29.193647\n",
      "Epoch 120 starting...\n",
      "Epoch 120 completed after 4:36:47.007902\n",
      "Epoch 121 starting...\n",
      "Epoch 121 completed after 4:39:06.678138\n",
      "Epoch 122 starting...\n",
      "Epoch 122 completed after 4:41:27.230113\n",
      "Epoch 123 starting...\n",
      "Epoch 123 completed after 4:43:46.145466\n",
      "Epoch 124 starting...\n",
      "Epoch 124 completed after 4:46:03.005978\n",
      "Epoch 125 starting...\n",
      "Epoch 125 completed after 4:48:19.775626\n",
      "Epoch 126 starting...\n",
      "Epoch 126 completed after 4:50:36.521716\n",
      "Epoch 127 starting...\n",
      "Epoch 127 completed after 4:52:53.171121\n",
      "Epoch 128 starting...\n",
      "Epoch 128 completed after 4:55:10.877549\n",
      "Epoch 129 starting...\n",
      "Epoch 129 completed after 4:57:30.658615\n",
      "Epoch 130 starting...\n",
      "Epoch 130 completed after 4:59:47.812899\n",
      "Epoch 131 starting...\n",
      "Epoch 131 completed after 5:02:05.305042\n",
      "Epoch 132 starting...\n",
      "Epoch 132 completed after 5:04:23.340192\n",
      "Epoch 133 starting...\n",
      "Epoch 133 completed after 5:06:41.374139\n",
      "Epoch 134 starting...\n",
      "Epoch 134 completed after 5:09:00.051359\n",
      "Epoch 135 starting...\n",
      "Epoch 135 completed after 5:11:18.477561\n",
      "Epoch 136 starting...\n",
      "Epoch 136 completed after 5:13:37.165648\n",
      "Epoch 137 starting...\n",
      "Epoch 137 completed after 5:15:55.872926\n",
      "Epoch 138 starting...\n",
      "Epoch 138 completed after 5:18:13.603486\n",
      "Epoch 139 starting...\n",
      "Epoch 139 completed after 5:20:31.633987\n",
      "Epoch 140 starting...\n",
      "Epoch 140 completed after 5:22:49.869901\n",
      "Epoch 141 starting...\n",
      "Epoch 141 completed after 5:25:11.436702\n",
      "Epoch 142 starting...\n",
      "Epoch 142 completed after 5:27:33.370874\n",
      "Epoch 143 starting...\n",
      "Epoch 143 completed after 5:29:53.744919\n",
      "Epoch 144 starting...\n",
      "Epoch 144 completed after 5:32:11.814251\n",
      "Epoch 145 starting...\n",
      "Epoch 145 completed after 5:34:30.321025\n",
      "Epoch 146 starting...\n",
      "Epoch 146 completed after 5:36:49.376708\n",
      "Epoch 147 starting...\n",
      "Epoch 147 completed after 5:39:09.887454\n",
      "Epoch 148 starting...\n",
      "Epoch 148 completed after 5:41:32.128769\n",
      "Epoch 149 starting...\n",
      "Epoch 149 completed after 5:43:53.318588\n",
      "Epoch 150 starting...\n",
      "Epoch 150 completed after 5:46:14.690868\n",
      "Epoch 151 starting...\n",
      "Epoch 151 completed after 5:48:36.291709\n",
      "Epoch 152 starting...\n",
      "Epoch 152 completed after 5:50:55.555219\n",
      "Epoch 153 starting...\n",
      "Epoch 153 completed after 5:53:14.828320\n",
      "Epoch 154 starting...\n",
      "Epoch 154 completed after 5:55:33.050221\n",
      "Epoch 155 starting...\n",
      "Epoch 155 completed after 5:57:50.907364\n",
      "Epoch 156 starting...\n",
      "Epoch 156 completed after 6:00:13.821561\n",
      "Epoch 157 starting...\n",
      "Epoch 157 completed after 6:02:30.898493\n",
      "Epoch 158 starting...\n",
      "Epoch 158 completed after 6:04:47.373958\n",
      "Epoch 159 starting...\n",
      "Epoch 159 completed after 6:07:04.668900\n",
      "Epoch 160 starting...\n",
      "Epoch 160 completed after 6:09:21.736707\n",
      "Epoch 161 starting...\n",
      "Epoch 161 completed after 6:11:37.984171\n",
      "Epoch 162 starting...\n",
      "Epoch 162 completed after 6:13:55.412100\n",
      "Epoch 163 starting...\n",
      "Epoch 163 completed after 6:16:14.152030\n",
      "Epoch 164 starting...\n",
      "Epoch 164 completed after 6:18:32.423909\n",
      "Epoch 165 starting...\n",
      "Epoch 165 completed after 6:20:50.035621\n",
      "Epoch 166 starting...\n",
      "Epoch 166 completed after 6:23:07.946000\n",
      "Epoch 167 starting...\n",
      "Epoch 167 completed after 6:25:25.255883\n",
      "Epoch 168 starting...\n",
      "Epoch 168 completed after 6:27:44.159522\n",
      "Epoch 169 starting...\n",
      "Epoch 169 completed after 6:30:01.673419\n",
      "Epoch 170 starting...\n",
      "Epoch 170 completed after 6:32:20.755675\n",
      "Epoch 171 starting...\n",
      "Epoch 171 completed after 6:34:40.329979\n",
      "Epoch 172 starting...\n",
      "Epoch 172 completed after 6:37:00.061426\n",
      "Epoch 173 starting...\n",
      "Epoch 173 completed after 6:39:16.932065\n",
      "Epoch 174 starting...\n",
      "Epoch 174 completed after 6:41:34.935617\n",
      "Epoch 175 starting...\n",
      "Epoch 175 completed after 6:43:52.154829\n",
      "Epoch 176 starting...\n",
      "Epoch 176 completed after 6:46:12.162294\n",
      "Epoch 177 starting...\n",
      "Epoch 177 completed after 6:48:34.120579\n",
      "Epoch 178 starting...\n",
      "Epoch 178 completed after 6:50:56.065897\n",
      "Epoch 179 starting...\n",
      "Epoch 179 completed after 6:53:13.373145\n",
      "Epoch 180 starting...\n",
      "Epoch 180 completed after 6:55:31.676147\n",
      "Epoch 181 starting...\n",
      "Epoch 181 completed after 6:57:49.454306\n",
      "Epoch 182 starting...\n",
      "Epoch 182 completed after 7:00:08.409847\n",
      "Epoch 183 starting...\n",
      "Epoch 183 completed after 7:02:29.217491\n",
      "Epoch 184 starting...\n",
      "Epoch 184 completed after 7:04:48.906331\n",
      "Epoch 185 starting...\n",
      "Epoch 185 completed after 7:07:08.120541\n",
      "Epoch 186 starting...\n",
      "Epoch 186 completed after 7:09:29.867746\n",
      "Epoch 187 starting...\n",
      "Epoch 187 completed after 7:11:52.362086\n",
      "Epoch 188 starting...\n",
      "Epoch 188 completed after 7:14:15.167827\n",
      "Epoch 189 starting...\n",
      "Epoch 189 completed after 7:16:37.503607\n",
      "Epoch 190 starting...\n",
      "Epoch 190 completed after 7:18:59.694214\n",
      "Epoch 191 starting...\n",
      "Epoch 191 completed after 7:21:21.831319\n",
      "Epoch 192 starting...\n",
      "Epoch 192 completed after 7:23:44.141170\n",
      "Epoch 193 starting...\n",
      "Epoch 193 completed after 7:26:06.786825\n",
      "Epoch 194 starting...\n",
      "Epoch 194 completed after 7:28:28.428627\n",
      "Epoch 195 starting...\n",
      "Epoch 195 completed after 7:30:50.005011\n",
      "Epoch 196 starting...\n",
      "Epoch 196 completed after 7:33:12.714295\n",
      "Epoch 197 starting...\n",
      "Epoch 197 completed after 7:35:35.597349\n",
      "Epoch 198 starting...\n",
      "Epoch 198 completed after 7:37:58.177686\n",
      "Epoch 199 starting...\n",
      "Epoch 199 completed after 7:40:19.414583\n",
      "Epoch 200 starting...\n",
      "Epoch 200 completed after 7:42:40.112946\n",
      "Epoch 201 starting...\n",
      "Epoch 201 completed after 7:44:58.844116\n",
      "Epoch 202 starting...\n",
      "Epoch 202 completed after 7:47:18.842821\n",
      "Epoch 203 starting...\n",
      "Epoch 203 completed after 7:49:38.972317\n",
      "Epoch 204 starting...\n",
      "Epoch 204 completed after 7:51:58.006406\n",
      "Epoch 205 starting...\n",
      "Epoch 205 completed after 7:54:15.608210\n",
      "Epoch 206 starting...\n",
      "Epoch 206 completed after 7:56:34.205255\n",
      "Epoch 207 starting...\n",
      "Epoch 207 completed after 7:58:52.531300\n",
      "Epoch 208 starting...\n",
      "Epoch 208 completed after 8:01:09.188152\n",
      "Epoch 209 starting...\n",
      "Epoch 209 completed after 8:03:28.147938\n",
      "Epoch 210 starting...\n",
      "Epoch 210 completed after 8:05:50.791158\n",
      "Epoch 211 starting...\n",
      "Epoch 211 completed after 8:08:11.477091\n",
      "Epoch 212 starting...\n",
      "Epoch 212 completed after 8:10:32.330201\n",
      "Epoch 213 starting...\n",
      "Epoch 213 completed after 8:12:49.446439\n",
      "Epoch 214 starting...\n",
      "Epoch 214 completed after 8:15:05.988395\n",
      "Epoch 215 starting...\n",
      "Epoch 215 completed after 8:17:22.376306\n",
      "Epoch 216 starting...\n",
      "Epoch 216 completed after 8:19:38.829682\n",
      "Epoch 217 starting...\n",
      "Epoch 217 completed after 8:21:56.191099\n",
      "Epoch 218 starting...\n",
      "Epoch 218 completed after 8:24:11.959853\n",
      "Epoch 219 starting...\n",
      "Epoch 219 completed after 8:26:27.426196\n",
      "Epoch 220 starting...\n",
      "Epoch 220 completed after 8:28:46.716593\n",
      "Epoch 221 starting...\n",
      "Epoch 221 completed after 8:31:07.815358\n",
      "Epoch 222 starting...\n",
      "Epoch 222 completed after 8:33:29.977127\n",
      "Epoch 223 starting...\n",
      "Epoch 223 completed after 8:35:47.119838\n",
      "Epoch 224 starting...\n",
      "Epoch 224 completed after 8:38:04.597599\n",
      "Epoch 225 starting...\n",
      "Epoch 225 completed after 8:40:21.848185\n",
      "Epoch 226 starting...\n",
      "Epoch 226 completed after 8:42:37.960746\n",
      "Epoch 227 starting...\n",
      "Epoch 227 completed after 8:44:54.218782\n",
      "Epoch 228 starting...\n",
      "Epoch 228 completed after 8:47:09.978865\n",
      "Epoch 229 starting...\n",
      "Epoch 229 completed after 8:49:27.399290\n",
      "Epoch 230 starting...\n",
      "Epoch 230 completed after 8:51:46.810950\n",
      "Epoch 231 starting...\n",
      "Epoch 231 completed after 8:54:09.810894\n",
      "Epoch 232 starting...\n",
      "Epoch 232 completed after 8:56:30.619776\n",
      "Epoch 233 starting...\n",
      "Epoch 233 completed after 8:58:51.657398\n",
      "Epoch 234 starting...\n",
      "Epoch 234 completed after 9:01:14.079245\n",
      "Epoch 235 starting...\n",
      "Epoch 235 completed after 9:03:33.620624\n",
      "Epoch 236 starting...\n",
      "Epoch 236 completed after 9:05:54.944476\n",
      "Epoch 237 starting...\n",
      "Epoch 237 completed after 9:08:17.113164\n",
      "Epoch 238 starting...\n",
      "Epoch 238 completed after 9:10:33.652973\n",
      "Epoch 239 starting...\n",
      "Epoch 239 completed after 9:12:51.218049\n",
      "Epoch 240 starting...\n",
      "Epoch 240 completed after 9:15:09.192940\n",
      "Epoch 241 starting...\n",
      "Epoch 241 completed after 9:17:26.925801\n",
      "Epoch 242 starting...\n",
      "Epoch 242 completed after 9:19:44.532882\n",
      "Epoch 243 starting...\n",
      "Epoch 243 completed after 9:22:01.912482\n",
      "Epoch 244 starting...\n",
      "Epoch 244 completed after 9:24:18.631126\n",
      "Epoch 245 starting...\n",
      "Epoch 245 completed after 9:26:36.310556\n",
      "Epoch 246 starting...\n",
      "Epoch 246 completed after 9:28:54.490223\n",
      "Epoch 247 starting...\n",
      "Epoch 247 completed after 9:31:10.330593\n",
      "Epoch 248 starting...\n",
      "Epoch 248 completed after 9:33:26.041955\n",
      "Epoch 249 starting...\n",
      "Epoch 249 completed after 9:35:41.716823\n",
      "Epoch 250 starting...\n",
      "Epoch 250 completed after 9:37:58.471364\n",
      "Epoch 251 starting...\n",
      "Epoch 251 completed after 9:40:15.124479\n",
      "Epoch 252 starting...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "generations = 1000\n",
    "hidden_layers = [128, 64, 32]\n",
    "mutation_rate = 0.15\n",
    "mutation_strength = 0.15\n",
    "\n",
    "folder = f\"networks/{'_'.join(str(x) for x in hidden_layers)}\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "print(f\"Saving networks to folder: {folder}\")\n",
    "\n",
    "scores_best = []\n",
    "\n",
    "optimizer = EvolutionaryOptimizer(\n",
    "    population_size=100,\n",
    "    elite_size=20,\n",
    "    new_members=100,\n",
    "    mutation_rate=mutation_rate,\n",
    "    mutation_strength=mutation_strength,\n",
    "    input_size=len(train_data[0]['input']),\n",
    "    hidden_layers=hidden_layers,\n",
    "    output_size=1,\n",
    "    train_data=train_data\n",
    ")\n",
    "\n",
    "def mutation_decay(x, mu0=0.25, k=3.0):\n",
    "    return mu0 * np.exp(-k * x)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "epoch = 0\n",
    "last_precision = 0\n",
    "while True:\n",
    "    print(f\"Epoch {epoch+1} starting...\")\n",
    "    epoch += 1\n",
    "    for gen in range(generations):\n",
    "        optimizer.mutation_strength = mutation_decay(\n",
    "            gen / generations, k=4, mu0=mutation_strength\n",
    "        )\n",
    "        optimizer.mutation_rate = mutation_decay(gen / generations, k=4, mu0=mutation_rate)\n",
    "\n",
    "        (population, best_precision) = optimizer.run_generation()\n",
    "        # print(f\"Epoch: {epoch} {gen+1}/{generations} - Best Precision: {best_precision:.4f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    duration = str(timedelta(seconds=(elapsed_time)))\n",
    "    print(f\"Epoch {epoch} completed after {duration}\")\n",
    "    if best_precision > last_precision:\n",
    "        save_population(optimizer.population, f\"{folder}/population_epoch_{epoch}.pkl\")\n",
    "        last_precision = best_precision    \n",
    "        print(f\"Saved population for epoch {epoch} with precision {best_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select network from saved checkpoint\n",
    "population = load_population('networks/128_64_32/population_epoch_15.pkl')\n",
    "network = population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select network from the optimizer\n",
    "network = optimizer.population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for test in test_data:\n",
    "    input_values = test['input']\n",
    "    prediction = network.forward(input_values)\n",
    "    # prediction = prediction.cpu()\n",
    "    prediction = True if prediction[0] > .5 else False\n",
    "    # print(f\"Passenger ID: {test['id']}, Survived: {1 if prediction else 0}\")\n",
    "    predictions.append([test['id'], 1 if prediction else 0])\n",
    "\n",
    "# Save predictions to CSV\n",
    "with open(\"submission.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"PassengerId\", \"Survived\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for pid, survived in predictions:\n",
    "        writer.writerow({\"PassengerId\": pid, \"Survived\": survived})\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "titanic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
