{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-29T10:21:23.812261Z",
     "iopub.status.busy": "2025-10-29T10:21:23.812022Z",
     "iopub.status.idle": "2025-10-29T10:21:24.572301Z",
     "shell.execute_reply": "2025-10-29T10:21:24.571407Z",
     "shell.execute_reply.started": "2025-10-29T10:21:23.812242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from datetime import timedelta\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "file_train_data = \"data/train.csv\"\n",
    "file_test_data = \"data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T10:30:25.431024Z",
     "iopub.status.busy": "2025-10-29T10:30:25.430630Z",
     "iopub.status.idle": "2025-10-29T10:30:25.442571Z",
     "shell.execute_reply": "2025-10-29T10:30:25.441503Z",
     "shell.execute_reply.started": "2025-10-29T10:30:25.430994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'PassengerId': '1', 'Survived': '0', 'Pclass': '3', 'Name': 'Braund, Mr. Owen Harris', 'Sex': 'male', 'Age': '22', 'SibSp': '1', 'Parch': '0', 'Ticket': 'A/5 21171', 'Fare': '7.25', 'Cabin': '', 'Embarked': 'S'}, {'PassengerId': '2', 'Survived': '1', 'Pclass': '1', 'Name': 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)', 'Sex': 'female', 'Age': '38', 'SibSp': '1', 'Parch': '0', 'Ticket': 'PC 17599', 'Fare': '71.2833', 'Cabin': 'C85', 'Embarked': 'C'}, {'PassengerId': '3', 'Survived': '1', 'Pclass': '3', 'Name': 'Heikkinen, Miss. Laina', 'Sex': 'female', 'Age': '26', 'SibSp': '0', 'Parch': '0', 'Ticket': 'STON/O2. 3101282', 'Fare': '7.925', 'Cabin': '', 'Embarked': 'S'}]\n",
      "[{'PassengerId': '892', 'Pclass': '3', 'Name': 'Kelly, Mr. James', 'Sex': 'male', 'Age': '34.5', 'SibSp': '0', 'Parch': '0', 'Ticket': '330911', 'Fare': '7.8292', 'Cabin': '', 'Embarked': 'Q'}, {'PassengerId': '893', 'Pclass': '3', 'Name': 'Wilkes, Mrs. James (Ellen Needs)', 'Sex': 'female', 'Age': '47', 'SibSp': '1', 'Parch': '0', 'Ticket': '363272', 'Fare': '7', 'Cabin': '', 'Embarked': 'S'}, {'PassengerId': '894', 'Pclass': '2', 'Name': 'Myles, Mr. Thomas Francis', 'Sex': 'male', 'Age': '62', 'SibSp': '0', 'Parch': '0', 'Ticket': '240276', 'Fare': '9.6875', 'Cabin': '', 'Embarked': 'Q'}]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "train_csv = []\n",
    "with open(file_train_data, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    for row in reader:\n",
    "        train_csv.append(row)\n",
    "\n",
    "print(train_csv[:3])\n",
    "\n",
    "test_csv = []\n",
    "with open(file_test_data, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    for row in reader:\n",
    "        test_csv.append(row)\n",
    "\n",
    "print(test_csv[:3])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T10:42:10.956467Z",
     "iopub.status.busy": "2025-10-29T10:42:10.956171Z",
     "iopub.status.idle": "2025-10-29T10:42:10.965620Z",
     "shell.execute_reply": "2025-10-29T10:42:10.964842Z",
     "shell.execute_reply.started": "2025-10-29T10:42:10.956447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '1', 'input': [1.0, 0.0, 0.275, 0.125, 0.0, 0.014151057562208049, 0.0], 'output': [0.0]}, {'id': '2', 'input': [0.0, 1.0, 0.475, 0.125, 0.0, 0.13913573538264068, 0.3333333333333333], 'output': [1.0]}, {'id': '3', 'input': [1.0, 1.0, 0.325, 0.0, 0.0, 0.015468569817999833, 0.0], 'output': [1.0]}]\n",
      "[{'id': '892', 'input': [1.0, 0.0, 0.45394736842105265, 0.0, 0.0, 0.015281580671177828, 1.0], 'output': [0.0]}, {'id': '893', 'input': [1.0, 1.0, 0.618421052631579, 0.125, 0.0, 0.013663090060062943, 0.0], 'output': [0.0]}, {'id': '894', 'input': [0.5, 0.0, 0.8157894736842105, 0.0, 0.0, 0.018908740708122825, 1.0], 'output': [0.0]}]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def parse_data(lines):\n",
    "    # First we filter out the data we need\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        data.append(\n",
    "            {\n",
    "                \"id\": l[\"PassengerId\"],\n",
    "                \"survived\": l.get(\"Survived\", 0),\n",
    "                \"class\": l[\"Pclass\"],\n",
    "                \"sex\": l[\"Sex\"],\n",
    "                \"age\": l[\"Age\"],\n",
    "                \"sibsp\": l[\"SibSp\"],\n",
    "                \"parch\": l[\"Parch\"],\n",
    "                \"fare\": l.get(\"Fare\", 0),\n",
    "                \"embarked\": l[\"Embarked\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Normalize data\n",
    "    norm_sex = {\"male\": 0, \"female\": 1}\n",
    "    norm_embarked = {\"S\": 0, \"C\": 1, \"Q\": 2, \"\": 3}\n",
    "\n",
    "    normalized_data = []\n",
    "    # Numberize values\n",
    "    for row in data:\n",
    "        # Check for errors\n",
    "        if norm_sex.get(row[\"sex\"], \"unknown\") == \"unknown\":\n",
    "            print(row[\"id\"], row[\"sex\"])\n",
    "        if norm_embarked.get(row[\"embarked\"], \"unknown\") == \"unknown\":\n",
    "            print(row[\"id\"], row[\"embarked\"])\n",
    "\n",
    "        nd = copy.deepcopy(row)\n",
    "\n",
    "        nd[\"sex\"] = norm_sex[row[\"sex\"]]\n",
    "        nd[\"fare\"] = float(row[\"fare\"]) if row[\"fare\"] != \"\" else 0\n",
    "        nd[\"embarked\"] = norm_embarked[row[\"embarked\"]]\n",
    "        nd[\"age\"] = float(row[\"age\"]) if row[\"age\"] != \"\" else 0\n",
    "\n",
    "        normalized_data.append(nd)\n",
    "\n",
    "\n",
    "    keys = list(normalized_data[0].keys())\n",
    "\n",
    "    # Extract min max of each key\n",
    "    keys_minmax = {}\n",
    "    for k in keys:\n",
    "        values = [d[k] for d in normalized_data if k in d]\n",
    "        # print(values)\n",
    "        min_value, max_value = float(min(values)), float(max(values))\n",
    "        keys_minmax[k] = {\"min\": min_value, \"max\": max_value}\n",
    "\n",
    "    # Normalize 0 to 1 all keys\n",
    "    for r in normalized_data:\n",
    "        for k in keys:\n",
    "            if k == \"id\":\n",
    "                continue\n",
    "            value = float(r[k])\n",
    "            minmax = keys_minmax[k]\n",
    "            norm_value = (value - minmax[\"min\"]) / (minmax[\"max\"] - minmax[\"min\"]) if (minmax[\"max\"] - minmax[\"min\"]) != 0 else 0.0\n",
    "            r[k] = norm_value\n",
    "            # print(keys_minmax[k], value, norm_value)\n",
    "\n",
    "    # Split data into input -> expected output\n",
    "    train_data = []\n",
    "    for r in normalized_data:\n",
    "        train_data.append(\n",
    "            {\n",
    "                \"id\": r[\"id\"],\n",
    "                \"input\": list(\n",
    "                    {k: v for k, v in r.items() if k not in [\"survived\", \"id\"]}.values()\n",
    "                ),\n",
    "                \"output\": [r[\"survived\"]],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return train_data\n",
    "\n",
    "train_data = parse_data(train_csv)\n",
    "print(train_data[:3])\n",
    "\n",
    "test_data = parse_data(test_csv)\n",
    "print(test_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T11:11:45.045237Z",
     "iopub.status.busy": "2025-10-29T11:11:45.044884Z",
     "iopub.status.idle": "2025-10-29T11:11:45.056497Z",
     "shell.execute_reply": "2025-10-29T11:11:45.055453Z",
     "shell.execute_reply.started": "2025-10-29T11:11:45.045206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Determine the best available device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = get_device()\n",
    "\n",
    "\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    \"\"\"Simple feedforward neural network using PyTorch\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 16,\n",
    "        hidden_layers: List[int] = [256],\n",
    "        output_size: int = 4,\n",
    "        empty: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if empty:\n",
    "            return\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Build layers using PyTorch modules\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Add hidden layers\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            # layers.append(nn.Sigmoid())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Add output layer (no activation)\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights using He initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "        # Move to device\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, nonlinearity=\"tanh\")\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Convert numpy array to tensor if needed and move to device\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x).float().to(DEVICE)\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            x = x.to(DEVICE)\n",
    "        elif isinstance(x, list):\n",
    "            x = torch.tensor(x, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        return self.network(x)\n",
    "\n",
    "    def mutate(self, mutation_rate: float = 0.1, mutation_strength: float = 0.5):\n",
    "        \"\"\"Mutate the network's weights and biases\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                if torch.rand(1).item() < mutation_rate:\n",
    "                    mutation = torch.randn_like(param) * mutation_strength\n",
    "                    param.add_(mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class EvolutionaryOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        population_size: int = 50,\n",
    "        elite_size: int = 10,\n",
    "        new_members: int = 0,\n",
    "        mutation_rate: float = 0.1,\n",
    "        mutation_strength: float = 0.5,\n",
    "        input_size: int = 8,\n",
    "        hidden_layers: List[int] = [32],\n",
    "        output_size: int = 1,\n",
    "        train_data: List = [],\n",
    "    ):\n",
    "        self.population_size = population_size\n",
    "        self.elite_size = elite_size\n",
    "        self.new_members = new_members\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.mutation_strength = mutation_strength\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.train_data = train_data\n",
    "\n",
    "        # Create initial population\n",
    "        self.population = []\n",
    "        for _ in range(population_size):\n",
    "            network = SimpleNeuralNetwork(\n",
    "                input_size=input_size,\n",
    "                output_size=output_size,\n",
    "                hidden_layers=hidden_layers,\n",
    "            )\n",
    "            self.population.append(network)\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "    ) -> List[Tuple[SimpleNeuralNetwork, int, float]]:\n",
    "        def eval_network(network: SimpleNeuralNetwork):\n",
    "            # Set network in eval mode\n",
    "            network.eval()\n",
    "            scores = []\n",
    "            with torch.no_grad():\n",
    "                for test in self.train_data:\n",
    "                    input_values = test['input']\n",
    "                    # print(input_values)\n",
    "                    prediction = network.forward(input_values)\n",
    "                    # prediction = prediction.cpu()\n",
    "                    prediction = True if prediction[0] > .5 else False\n",
    "                    # print(prediction)\n",
    "                    reality = True if test['output'][0] == 1.0 else False\n",
    "                    # print(test['output'], reality)\n",
    "                    scores.append(1 if prediction == reality else 0)\n",
    "\n",
    "            return (network, sum(scores) / len(scores))\n",
    "\n",
    "\n",
    "        results = Parallel(n_jobs=joblib.cpu_count())(\n",
    "            delayed(eval_network)(net) for net in self.population\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def select_and_breed(\n",
    "        self, evaluated: List[Tuple[SimpleNeuralNetwork, int, float]]\n",
    "    ) -> None:\n",
    "        # Sort by score descending\n",
    "        evaluated.sort(key=lambda x: x[1], reverse=True)\n",
    "        elite = evaluated[: self.elite_size]\n",
    "\n",
    "        new_population = []\n",
    "        # Keep elite networks\n",
    "        for net, _ in elite:\n",
    "            new_population.append(net)\n",
    "\n",
    "        # Create offspring by mutating elite networks\n",
    "        while len(new_population) < self.population_size:\n",
    "            parent = random.choice(elite)[0]\n",
    "\n",
    "            # Create a child by copying the parent's state\n",
    "            child = copy.deepcopy(parent)\n",
    "\n",
    "            # Mutate the child\n",
    "            child.mutate(self.mutation_rate, self.mutation_strength)\n",
    "            new_population.append(child)\n",
    "\n",
    "        # Add random new members\n",
    "        for _ in range(self.new_members):\n",
    "            network = SimpleNeuralNetwork(hidden_layers=self.hidden_layers, input_size=self.input_size, output_size=self.output_size)\n",
    "            new_population.append(network)\n",
    "\n",
    "        self.population = new_population\n",
    "\n",
    "    def run_generation(\n",
    "        self,\n",
    "    ) -> Tuple[List[SimpleNeuralNetwork], float]:\n",
    "        evaluated = self.evaluate()\n",
    "        best_precision = max(precision for _, precision in evaluated)\n",
    "\n",
    "        self.select_and_breed(evaluated)\n",
    "\n",
    "        return self.population, best_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_network(network: SimpleNeuralNetwork, filename: str):\n",
    "    torch.save(network.state_dict(), filename)\n",
    "\n",
    "\n",
    "def load_network(filename: str, hidden_layers: List[int]) -> SimpleNeuralNetwork:\n",
    "    network = SimpleNeuralNetwork(hidden_layers=hidden_layers)\n",
    "    network.load_state_dict(torch.load(filename, map_location=DEVICE))\n",
    "    network.to(DEVICE)\n",
    "    return network\n",
    "\n",
    "\n",
    "def save_population(population: List[SimpleNeuralNetwork], filename: str):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(population, f)\n",
    "\n",
    "\n",
    "def load_population(filename: str) -> List[SimpleNeuralNetwork]:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        population = pickle.load(f)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:19:47.931825Z",
     "iopub.status.busy": "2025-10-29T12:19:47.931490Z",
     "iopub.status.idle": "2025-10-29T12:19:47.939072Z",
     "shell.execute_reply": "2025-10-29T12:19:47.938169Z",
     "shell.execute_reply.started": "2025-10-29T12:19:47.931801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving networks to folder: networks/16_16_16\n",
      "Epoch 1 starting...\n",
      "Epoch: 1 1/10 - Best Precision: 0.7868\n",
      "Epoch: 1 2/10 - Best Precision: 0.7879\n",
      "Epoch: 1 3/10 - Best Precision: 0.7969\n",
      "Epoch: 1 4/10 - Best Precision: 0.7969\n",
      "Epoch: 1 5/10 - Best Precision: 0.8002\n",
      "Epoch: 1 6/10 - Best Precision: 0.8025\n",
      "Epoch: 1 7/10 - Best Precision: 0.8036\n",
      "Epoch: 1 8/10 - Best Precision: 0.8047\n",
      "Epoch: 1 9/10 - Best Precision: 0.8058\n",
      "Epoch: 1 10/10 - Best Precision: 0.8058\n",
      "Epoch 1 completed after 0:01:00.361474\n",
      "Epoch 2 starting...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "generations = 10\n",
    "hidden_layers = [16, 16, 16]\n",
    "mutation_rate = 0.5\n",
    "mutation_strength = 0.25\n",
    "\n",
    "folder = f\"networks/{'_'.join(str(x) for x in hidden_layers)}\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "print(f\"Saving networks to folder: {folder}\")\n",
    "\n",
    "scores_best = []\n",
    "\n",
    "optimizer = EvolutionaryOptimizer(\n",
    "    population_size=100,\n",
    "    elite_size=20,\n",
    "    new_members=0,\n",
    "    mutation_rate=mutation_rate,\n",
    "    mutation_strength=mutation_strength,\n",
    "    input_size=7,\n",
    "    hidden_layers=hidden_layers,\n",
    "    output_size=1,\n",
    "    train_data=train_data\n",
    ")\n",
    "\n",
    "def mutation_decay(x, mu0=0.25, k=3.0):\n",
    "    return mu0 * np.exp(-k * x)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "epoch = 0\n",
    "while True:\n",
    "    print(f\"Epoch {epoch+1} starting...\")\n",
    "    epoch += 1\n",
    "    for gen in range(generations):\n",
    "        optimizer.mutation_strength = mutation_decay(\n",
    "            gen / generations, k=4, mu0=mutation_strength\n",
    "        )\n",
    "        optimizer.mutation_rate = mutation_decay(gen / generations, k=4, mu0=mutation_rate)\n",
    "\n",
    "        (population, best_precision) = optimizer.run_generation()\n",
    "        print(f\"Epoch: {epoch} {gen+1}/{generations} - Best Precision: {best_precision:.4f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    duration = str(timedelta(seconds=(elapsed_time)))\n",
    "    print(f\"Epoch {epoch} completed after {duration}\")\n",
    "    save_population(optimizer.population, f\"{folder}/population_epoch_{epoch}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction\n",
    "network = optimizer.population[0]\n",
    "\n",
    "predictions = []\n",
    "for test in test_data:\n",
    "    input_values = test['input']\n",
    "    prediction = network.forward(input_values)\n",
    "    prediction = prediction.cpu()\n",
    "    prediction = True if prediction[0] > .5 else False\n",
    "    # print(f\"Passenger ID: {test['id']}, Survived: {1 if prediction else 0}\")\n",
    "    predictions.append([test['id'], 1 if prediction else 0])\n",
    "\n",
    "# Save predictions to CSV\n",
    "with open(\"submission.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"PassengerId\", \"Survived\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for pid, survived in predictions:\n",
    "        writer.writerow({\"PassengerId\": pid, \"Survived\": survived})\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "titanic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
